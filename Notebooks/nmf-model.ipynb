{"cells":[{"metadata":{},"cell_type":"markdown","source":"# **Comet Experiment**"},{"metadata":{"trusted":false},"cell_type":"code","source":"# Install the comet_ml if the code below is missing the package\n# !pip install comet_ml","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"\n# import comet_ml at the top of your file\nfrom comet_ml import Experiment\n\n# Create an experiment with your api key:\nexperiment = Experiment(\n    api_key=\"ZBRB8H2ncCGZsUoS6CqVIAr0y\",\n    project_name=\"general\",\n    workspace=\"knetshiongolwe\",\n)\n\n\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n\n# **Import libraries and datasets**\n\nWe will be working with the famous Surprise(Simple Python RecommendatIon System Engine.) Library, Surprise is a Python scikit for building and analyzing recommender systems that deal with explicit rating data. \n\nBelow are all libraries that are used through out this notebook."},{"metadata":{"trusted":true},"cell_type":"code","source":"# data analysis libraries\nimport pandas as pd\nimport numpy as np\n\n# visualisation libraries\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom numpy.random import RandomState\n\n\n# Notebook styling\n%matplotlib inline\nsns.set()\n\n\n# ML Models\nfrom surprise import Reader\nfrom surprise import Dataset\nfrom surprise.model_selection import cross_validate\nfrom surprise import NMF\nfrom surprise.accuracy import rmse\nfrom surprise import accuracy\n\n# ML Pre processing\nfrom surprise.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Hyperparameter tuning\nfrom surprise.model_selection import GridSearchCV\n\n# High performance hyperparameter tuning\n#from tune_sklearn import TuneSearchCV\n#import warnings\n#warnings.filterwarnings(\"ignore\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Loading data**\n<a id=\"section-two\"></a>\n\nWe will load all the dataframes that we desire to work with "},{"metadata":{"trusted":true},"cell_type":"code","source":"#Pandas libraries used in the notebook.\ntrain = pd.read_csv('/kaggle/input/edsa-recommender-system-predict/train.csv')\ntest_df = pd.read_csv('/kaggle/input/edsa-recommender-system-predict/test.csv')\ndf_movies = pd.read_csv('/kaggle/input/edsa-recommender-system-predict/movies.csv')\n# df_samp = pd.read_csv('/kaggle/input/edsa-recommender-system-predict/sample_submission.csv')\n# df_imdb = pd.read_csv('/kaggle/input/edsa-recommender-system-predict/imdb_data.csv')\n# df_gtags = pd.read_csv(\"/kaggle/input/edsa-recommender-system-predict/genome_tags.csv\")\n# df_scores = pd.read_csv(\"/kaggle/input/edsa-recommender-system-predict/genome_scores.csv\")\n# df_tags = pd.read_csv(\"/kaggle/input/edsa-recommender-system-predict/tags.csv\")\n# df_links = pd.read_csv(\"/kaggle/input/edsa-recommender-system-predict/links.csv\")\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# **Evaluating Data**\n\nHere is the data that we are was given to us.\nSupplied Files\n*\n* train.csv - The training split of the dataset. Contains user and movie IDs with associated rating data.\n\n*Description of the data that is given to us *"},{"metadata":{"trusted":true},"cell_type":"code","source":"#viewing training data\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Train:\n\n* UserId\n* movieId : Identifier for movies used\n* rating : Ratings are made on a 5-star scale, with half-star increments (0.5 stars - 5.0 stars).\n* timestamp: represent seconds since midnight Coordinated Universal Time (UTC) of January 1, 1970."},{"metadata":{"trusted":true},"cell_type":"code","source":"#Viewing movies data\ndf_movies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Movies:\n\n* movieId : Identifier for movies used\n\n* title : These were entered manually or imported from https://www.themoviedb.org/, and include the year of release in parentheses. Errors and inconsistencies may exist in these titles.\n\n* genres: Genres are a pipe-separated list, and are selected from the following:\n\n    * Action\n    * Adventure\n    * Animation\n    * Children's\n    * Comedy\n    * Crime\n    * Documentary\n    * Drama\n    * Fantasy\n    * Film-Noir\n    * Horror\n    * Musical\n    * Mystery\n    * Romance\n    * Sci-Fi\n    * Thriller\n    * War\n    * Western\n    * (no genres listed)"},{"metadata":{},"cell_type":"markdown","source":"<a id=\"section-four\"></a>\n# **Data Preprocessing**\n**Preparing raw data:**\n\nWe will first prepare this raw data to make it suitable for our machine learning model. This is a very crucial step while for creating a machine learning model."},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-one\"></a>\n# **Checking for missing values column wise**\n\n**Handling Missing Data:**\n\nIn our dataset, there may be some missing values. We cannot train our model with a dataset that contains missing values. So we have to check if our dataset has missing values.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"#check for missing values\ntrain.isnull().sum()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-two\"></a>\n# **Checking for duplicates records**\n**Checking Duplicate Values:**\n\nAt times our dataset may entail some duplicated values which are not necessary therefore this values must be removed, befor removing these duplicates we are able to first check if we do have them. We will implement this by the code below."},{"metadata":{"trusted":true},"cell_type":"code","source":"#check duplicates\ndup_bool = train.duplicated(['userId','movieId','rating'])\n\n#display duplicates\nprint(\"Number of duplicate records:\",sum(dup_bool))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"<a id=\"subsection-three\"></a>\n# **Creating a copy**\n\nWe will rename our train data as df and look at the top 5 records in the dataframe."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train.copy()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#create a copy of the train data\ndf_train = train.copy()\n\n#display top 5 records\ndf_train.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Evaluating Length of Unique Values**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Find the length of the unique use\nlen(df_train['userId'].unique()), len(df_train['movieId'].unique())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#view movies\ndf_movies.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Joining Datasets**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Merge\ndf_merge1 = df_train.merge(df_movies, on = 'movieId')\ndf_merge1.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# **Collaborative Filtering**\n\n**What Is Collaborative Filtering?**\n\nCollaborative filtering is a technique that can filter out items that a user might like on the basis of reactions by similar users.\n\nIt works by searching a large group of people and finding a smaller set of users with tastes similar to a particular user. It looks at the items they like and combines them to create a ranked list of suggestions.\n\nto be more precise it is based on similarity in preference , taste and choices of two users. A good example that we can give you could be if user A likes movies 1,2 and 3 and user B likes movies 2,3 and 4 then this implies that they have similar interests and user A should like movie 4 and B should like movie 1.\n\n\n**Why Do We Consider Collaborating Filtering Over Content Based Filtering?**\n\nCollaborative filtering recommender engine is a much better algorithim then content content based filtering since it is able to do feature laerning on its own, in other words it can laern which features to use\n\n**Advantages of Collaborative filtering:**\n\nTaken that we find collaborative filtering better than content based, We will give a few adavntages to support the argument.\n\n* Takes other user ratings into consideration \n* Doesnt need to study or extract information from recommended item.\n* It adapts to the user' interest which might change over time.\n\n**About Collaborative Filtering Datasets:**\n\nTo take note that in order for us to implement this algorithm or any recommendation algorithms  we need a specific dataset that is stuctured in a specific format. This data should entail a set of items and users who have reacted to some of the items.\n\nWhile working with such data, you’ll mostly see it in the form of a matrix consisting of the reactions given by a set of users to some items from a set of items. Each row would contain the ratings given by a user, and each column would contain the ratings received by an item. A matrix with five users and five items could look like this:\n\n\n**Rating Matrix:**\n\n\n![](https://files.realpython.com/media/rating-matrix.04153775e4c1.jpg)\n\nThe matrix shows five users who have rated some of the items on a scale of 1 to 5. For example, the first user has given a rating 4 to the third item. n most cases, the cells in the matrix are empty, as users only rate a few items. It’s highly unlikely for every user to rate or react to every item available. A matrix with mostly empty cells is called sparse, and the opposite to that (a mostly filled matrix) is called dense.\n\n**How do you measure the accuracy of the ratings you calculate?**\n\nEsentially there are many approaches but we will explain the main approach that we will need for this project which is the Root Mean Square Error (RMSE), in which you predict ratings for a test dataset of user-item pairs whose rating values are already known. The difference between the known value and the predicted value would be the error. Square all the error values for the test set, find the average (or mean), and then take the square root of that average to get the RMSE.\n\n![](https://www.analyticsvidhya.com/wp-content/uploads/2016/02/rmse.png)\n\nAnother metric to measure the accuracy is Mean Absolute Error (MAE), in which you find the magnitude of error by finding its absolute value and then taking the average of all error values.\n\nHowever we will be focusing on the RMSE for our predictions.\n\nBefore diving deep into the code we would like to clarify the Type of collaborative filtering we are going to implement. \n\nRecommender Sysem is divided ito Three brances of which collaborative filtering is entailed, the figure below will make a clear breakdown to the reader.\n\n![](https://www.seoclerk.com/pics/want61009-1nSWOn1525162745.jpg)\n\nYou will notice that Collaborative filtering consist of two filtering techniques, \n\n* **Model-based Technique**\n* **Memory-based filtering**\n\nWe will give a short description of these techniques. \n\n* **Model-based Technique**\nModel based collaborative filtering algorithms provide item recommendations by first developing a model of user ratings. With these systems you will build a model from user ratings and then make recommendations based on that model, this offers a speed and scalability that not available when\nyoure forced to refer back to the entire dataset to make a prediction.\n\n* **Memory based filtering**\nMemory based rely heavely on simple similarity measures(cosine similarity, pearson correlation and more) to match similar people or items together.\nthses consist of two methods namely **Item based** and **user based** collaborative filtering.\n\nThe figure below defines the two filtering methods.\n\n![](https://cdn-images-1.medium.com/max/1600/1*7uW5hLXztSu_FOmZOWpB6g.png)"},{"metadata":{},"cell_type":"markdown","source":" ### **Loading as Surprise Dataframe**\n\nWe will be using the dataset module which loads the pandas dataframe that is available for this experiment, The reader function is used to parse a file containing ratings data. The default format in which it accepts data is \nthat each rating is stored in a separate line in the order user, movie and rating"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Loading as Surprise dataframe \nreader = Reader(rating_scale=(0.5, 5))\n\ndata = Dataset.load_from_df(df_train[['userId', 'movieId' ,'rating']], reader)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Splitting into Train and Test Set**\n\nAnother way to implement the sampling of the trainset and testset without the use of a cross validate procedure is by using a train test split given the sizes , with the acuracy metrics of choice.\n\nWe use a random trainset and testset with the testset that is  15% of the ratings."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Data split 85/15\ntrainset, testset = train_test_split(data, test_size=0.15)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Training  Model**\n\nUsing the base algoritm of `Non-Negative Matrix Factorization` we will fit method which will train the algorithm on the trainset and and the test() method which will return the predictions made from the testset furthermore storing all our predictions on a dataframe called test."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Base algorithm\nnmf_algo = NMF()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fitting our trainset\nnmf_algo.fit(trainset)\n\n# Using the 15% testset to make predictions\npredictions = nmf_algo.test(testset) \npredictions\n\ntest = pd.DataFrame(predictions)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let us have a closer look into the predictions on the dataframe test."},{"metadata":{"trusted":true},"cell_type":"code","source":"# View the head\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Evaluate Model**\n\nUtilising the test dataframe that we have created we are going to predict some of the ratings for each userId and movieId pair, this ratings predictions will be collected and stored as a list consiting of these pairs, ideally this list will help in predicting unknown values in the original matrix(test_df dataframe) (this is also known as matrix completion)\n\nLet us look at the list called ratings predictions."},{"metadata":{"trusted":true},"cell_type":"code","source":"# We are trying to predict ratings for every userId / movieId pair, we implement the below list comprehension to achieve this.\nratings_predictions=[nmf_algo.predict(row.userId, row.movieId) for _,row in test_df.iterrows()]\nratings_predictions","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We will store the list of predictions in a dataframe which will essentially in help in creating the familiar format of the dataframe"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Converting our prediction into a familiar format-Dataframe\ndf_pred=pd.DataFrame(ratings_predictions)\ndf_pred","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Renaming our predictions to original names\ndf_pred=df_pred.rename(columns={'uid':'userId', 'iid':'movieId','est':'rating'})\ndf_pred.drop(['r_ui','details'],axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Snippet of our ratings\ndf_pred.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Concatenating userId/movieId into a single Id column.(code has to be run twice to get desired outcome)\ndf_pred['Id']=df_pred.apply(lambda x:'%s_%s' % (x['userId'],x['movieId']),axis=1)\ndf_pred['Id']=df_pred.apply(lambda x:'%s_%s' % (x['userId'],x['movieId']),axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# drop the two features from the dataset userId and movieId\ndf_pred.drop(['userId', 'movieId'], inplace=True, axis= 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## **Preparing Submission**\n\nThe submission of this competition has to be in csv file entailing a id and rating column"},{"metadata":{"trusted":false},"cell_type":"code","source":"# df_pred = df_pred[['Id', 'rating']]\n# df_pred.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"# df_pred.to_csv(\"nmf_model_base.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# IMPORTANT NOTE"},{"metadata":{},"cell_type":"markdown","source":"In this notebook: This model is implemented on sample dataset, however, with the same procedure this model was implemented on a notebook on kaggle but because of memory issues this notebook was then moved here to GitHub. Another issues arises when implementing the notebook because of the huge dataset. This issue can be resolved in the three following steps. \n1. Clone this repository.\n2. Go to kaggle and download the dataset on: https://www.kaggle.com/c/edsa-recommender-system-predict/data\n3. Replace the data on the folder titled 'data' with three dataset loaded in this notebook.\n4. If you run this whole notebook you it will return a csv file title `coClustering_model_base.csv`. \n5. Uploading that file on kaggle will give a score of"},{"metadata":{"trusted":false},"cell_type":"code","source":"experiment.end()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}
