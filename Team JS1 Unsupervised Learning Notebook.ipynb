{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.084656,
     "end_time": "2021-07-12T20:57:03.191518",
     "exception": false,
     "start_time": "2021-07-12T20:57:03.106862",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# EDSA Movie Recommendation challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T01:16:03.523186Z",
     "iopub.status.busy": "2021-07-12T01:16:03.522889Z",
     "iopub.status.idle": "2021-07-12T01:16:03.527928Z",
     "shell.execute_reply": "2021-07-12T01:16:03.527226Z",
     "shell.execute_reply.started": "2021-07-12T01:16:03.52316Z"
    },
    "papermill": {
     "duration": 0.080527,
     "end_time": "2021-07-12T20:57:03.353470",
     "exception": false,
     "start_time": "2021-07-12T20:57:03.272943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<img src=\"https://media.giphy.com/media/3ohhwDMC187JqL69DG/giphy.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.080614,
     "end_time": "2021-07-12T20:57:03.515435",
     "exception": false,
     "start_time": "2021-07-12T20:57:03.434821",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.10072,
     "end_time": "2021-07-12T20:57:03.697353",
     "exception": false,
     "start_time": "2021-07-12T20:57:03.596633",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "In Today's technology driven world,recommender systems are socially and economically critical for ensuring that idividuals can make appropriate choices surrounding the content they engage wit on a daily basis. One application where this is especially true surrounds movie content recommendations;where intelligent algorithms can help viewers find great titles from tens of thousands of movies\n",
    "\n",
    "With this context,we were able to construct a recommendation algorithm based on content or collaborative filtering,cabable of accurately predicting how a user will rate a movie theyhave not  yet viewed based on their historical preferences.\n",
    "\n",
    "Providing an accurate and robust solution to this challenge has immense economic potential,with users of the system being exposed to the content they would like to view or purchase- generating revenue and platform affinity\n",
    "\n",
    "Authors:Brighton Nkomo,Claudia Elliot-Wilson,Eve Sithhole,Reneilwe Mphahlele,Vuyiswa M"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.08784,
     "end_time": "2021-07-12T20:57:03.869179",
     "exception": false,
     "start_time": "2021-07-12T20:57:03.781339",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Table of contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-08T10:23:47.795973Z",
     "iopub.status.busy": "2021-07-08T10:23:47.795652Z",
     "iopub.status.idle": "2021-07-08T10:23:47.8017Z",
     "shell.execute_reply": "2021-07-08T10:23:47.800244Z",
     "shell.execute_reply.started": "2021-07-08T10:23:47.795946Z"
    },
    "papermill": {
     "duration": 0.086004,
     "end_time": "2021-07-12T20:57:04.044605",
     "exception": false,
     "start_time": "2021-07-12T20:57:03.958601",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "1. Install packages\n",
    "2. Reading in data\n",
    "3. Data Pre-processing\n",
    "4. Exploratory Data Analysis\n",
    "5. Feature Engineering\n",
    "6. Model Selection\n",
    "7. Submission\n",
    "8. Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T20:57:04.215416Z",
     "iopub.status.busy": "2021-07-12T20:57:04.213567Z",
     "iopub.status.idle": "2021-07-12T20:57:04.231962Z",
     "shell.execute_reply": "2021-07-12T20:57:04.230983Z",
     "shell.execute_reply.started": "2021-07-12T20:00:04.655392Z"
    },
    "papermill": {
     "duration": 0.104835,
     "end_time": "2021-07-12T20:57:04.232213",
     "exception": false,
     "start_time": "2021-07-12T20:57:04.127378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081572,
     "end_time": "2021-07-12T20:57:04.396330",
     "exception": false,
     "start_time": "2021-07-12T20:57:04.314758",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Installing packages\n",
    "Please download all relevant packages in. There is no terminal so you will pip install everything.\n",
    "\n",
    "You can find a list of recommended install from the Intro to Recommender sysytem notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T20:57:04.590039Z",
     "iopub.status.busy": "2021-07-12T20:57:04.589120Z",
     "iopub.status.idle": "2021-07-12T20:57:05.916491Z",
     "shell.execute_reply": "2021-07-12T20:57:05.917305Z",
     "shell.execute_reply.started": "2021-07-12T20:00:18.367624Z"
    },
    "papermill": {
     "duration": 1.427917,
     "end_time": "2021-07-12T20:57:05.917530",
     "exception": false,
     "start_time": "2021-07-12T20:57:04.489613",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install packages here\n",
    "# Packages for data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from sklearn import preprocessing\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import scipy as sp\n",
    "\n",
    "# Packages for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Packages for modeling\n",
    "from surprise import Reader\n",
    "from surprise import Dataset\n",
    "from surprise import KNNWithMeans\n",
    "from surprise import KNNBasic\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise.model_selection import GridSearchCV\n",
    "from surprise import SVD\n",
    "from surprise import SVDpp\n",
    "from surprise import NMF\n",
    "from surprise import SlopeOne\n",
    "from surprise import CoClustering\n",
    "from surprise import NormalPredictor, BaselineOnly\n",
    "import heapq\n",
    "\n",
    "# Packages for model evaluation\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from time import time\n",
    "\n",
    "# Package to suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Packages for saving models\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.082195,
     "end_time": "2021-07-12T20:57:06.082149",
     "exception": false,
     "start_time": "2021-07-12T20:57:05.999954",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Reading in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T11:19:50.540712Z",
     "iopub.status.busy": "2021-06-14T11:19:50.540043Z",
     "iopub.status.idle": "2021-06-14T11:20:08.370886Z",
     "shell.execute_reply": "2021-06-14T11:20:08.371333Z",
     "shell.execute_reply.started": "2021-06-06T09:35:50.49335Z"
    },
    "papermill": {
     "duration": 17.847754,
     "end_time": "2021-06-14T11:20:08.371525",
     "exception": false,
     "start_time": "2021-06-14T11:19:50.523771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sample_submission = pd.read_csv('sample_submission.csv')\n",
    "df_movies = pd.read_csv('movies.csv')\n",
    "df_imdb = pd.read_csv('imdb_data.csv')\n",
    "df_genome_scores = pd.read_csv('genome_scores.csv')\n",
    "df_genome_tags = pd.read_csv('genome_tags.csv')\n",
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_tags = pd.read_csv('tags.csv')\n",
    "df_links = pd.read_csv('links.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.082426,
     "end_time": "2021-07-12T20:57:31.331853",
     "exception": false,
     "start_time": "2021-07-12T20:57:31.249427",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T20:57:31.509348Z",
     "iopub.status.busy": "2021-07-12T20:57:31.508430Z",
     "iopub.status.idle": "2021-07-12T20:57:31.869995Z",
     "shell.execute_reply": "2021-07-12T20:57:31.869326Z",
     "shell.execute_reply.started": "2021-07-12T20:15:59.587955Z"
    },
    "papermill": {
     "duration": 0.45496,
     "end_time": "2021-07-12T20:57:31.870175",
     "exception": false,
     "start_time": "2021-07-12T20:57:31.415215",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Using regular expressions to find a year stored between parentheses\n",
    "#We specify the parantheses so we don't conflict with movies that have years in their titles\n",
    "df_movies['year'] = df_movies.title.str.extract('(\\(\\d\\d\\d\\d\\))',expand=False)\n",
    "#Removing the parentheses\n",
    "df_movies['year'] = df_movies.year.str.extract('(\\d\\d\\d\\d)',expand=False)\n",
    "#Removing the years from the 'title' column\n",
    "df_movies['title'] = df_movies.title.str.replace('(\\(\\d\\d\\d\\d\\))', '')\n",
    "#Applying the strip function to get rid of any ending whitespace characters that may have appeared\n",
    "df_movies['title'] = df_movies['title'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T20:57:32.049104Z",
     "iopub.status.busy": "2021-07-12T20:57:32.048066Z",
     "iopub.status.idle": "2021-07-12T20:57:32.292844Z",
     "shell.execute_reply": "2021-07-12T20:57:32.292085Z",
     "shell.execute_reply.started": "2021-07-12T20:16:09.625589Z"
    },
    "papermill": {
     "duration": 0.340557,
     "end_time": "2021-07-12T20:57:32.293018",
     "exception": false,
     "start_time": "2021-07-12T20:57:31.952461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Every genre is separated by a | so we simply have to call the split function on |\n",
    "df_movies['genres'] = df_movies.genres.str.split('|')\n",
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.082694,
     "end_time": "2021-07-12T20:57:32.457707",
     "exception": false,
     "start_time": "2021-07-12T20:57:32.375013",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.08409,
     "end_time": "2021-07-12T20:57:32.626059",
     "exception": false,
     "start_time": "2021-07-12T20:57:32.541969",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Title Wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081612,
     "end_time": "2021-07-12T20:57:32.790169",
     "exception": false,
     "start_time": "2021-07-12T20:57:32.708557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Are there certain words that figure more often in Movie Titles? There are some words which are considered more potent and considered more worthy of a title. Let us find out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T20:57:32.976962Z",
     "iopub.status.busy": "2021-07-12T20:57:32.976240Z",
     "iopub.status.idle": "2021-07-12T20:57:32.979565Z",
     "shell.execute_reply": "2021-07-12T20:57:32.978876Z",
     "shell.execute_reply.started": "2021-07-12T20:04:28.261017Z"
    },
    "papermill": {
     "duration": 0.107408,
     "end_time": "2021-07-12T20:57:32.979753",
     "exception": false,
     "start_time": "2021-07-12T20:57:32.872345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_movies['title'] = df_movies['title'].astype('str')\n",
    "title_corpus = ' '.join(df_movies['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T20:57:33.279884Z",
     "iopub.status.busy": "2021-07-12T20:57:33.279144Z",
     "iopub.status.idle": "2021-07-12T20:58:01.056182Z",
     "shell.execute_reply": "2021-07-12T20:58:01.056702Z",
     "shell.execute_reply.started": "2021-07-12T20:04:46.005552Z"
    },
    "papermill": {
     "duration": 27.993,
     "end_time": "2021-07-12T20:58:01.056907",
     "exception": false,
     "start_time": "2021-07-12T20:57:33.063907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "title_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', height=2000, width=4000).generate(title_corpus)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(title_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.093005,
     "end_time": "2021-07-12T20:58:01.244225",
     "exception": false,
     "start_time": "2021-07-12T20:58:01.151220",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The word Love is the most commonly used word in movie titles. Girl, Day and Man are also among the most commonly occuring words. We think this encloses the idea of the universal presence of romance in movies pretty well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.094421,
     "end_time": "2021-07-12T20:58:01.432399",
     "exception": false,
     "start_time": "2021-07-12T20:58:01.337978",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Numbers of movies by year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.0924,
     "end_time": "2021-07-12T20:58:01.619124",
     "exception": false,
     "start_time": "2021-07-12T20:58:01.526724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The dataset available to us does not present an entire collection of movies released since the inception of cinema. However, it is reasonable to assume that it does include almost every major film released in Hollywood as well as other major film industries across the world (such as Bollywood in India). With this assumption in mind, let us take a look at the number of movies produced by the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T20:58:01.808052Z",
     "iopub.status.busy": "2021-07-12T20:58:01.807270Z",
     "iopub.status.idle": "2021-07-12T20:58:02.036862Z",
     "shell.execute_reply": "2021-07-12T20:58:02.037331Z",
     "shell.execute_reply.started": "2021-07-12T20:16:38.075226Z"
    },
    "papermill": {
     "duration": 0.325731,
     "end_time": "2021-07-12T20:58:02.037538",
     "exception": false,
     "start_time": "2021-07-12T20:58:01.711807",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#checking numbers of movies by year\n",
    "year_count = df_movies.groupby('year')['title'].count()\n",
    "plt.figure(figsize=(18,5))\n",
    "year_count.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.095889,
     "end_time": "2021-07-12T20:58:02.231676",
     "exception": false,
     "start_time": "2021-07-12T20:58:02.135787",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We notice that there is a sharp rise in the number of movies starting the 1990s decade. However, we will not look too much into this as it is entirely possible that recent movies were oversampled for the purposes of this dataset.\n",
    "\n",
    "Next, let us take a look at the earliest movies represented in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T20:58:02.425369Z",
     "iopub.status.busy": "2021-07-12T20:58:02.424683Z",
     "iopub.status.idle": "2021-07-12T20:58:02.559706Z",
     "shell.execute_reply": "2021-07-12T20:58:02.560188Z",
     "shell.execute_reply.started": "2021-07-12T20:18:19.887199Z"
    },
    "papermill": {
     "duration": 0.233578,
     "end_time": "2021-07-12T20:58:02.560398",
     "exception": false,
     "start_time": "2021-07-12T20:58:02.326820",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_movies[df_movies['year'] != 'NaT'][['title', 'year']].sort_values('year').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.095542,
     "end_time": "2021-07-12T20:58:02.751399",
     "exception": false,
     "start_time": "2021-07-12T20:58:02.655857",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The oldest movie, Passage of Venus, was a series of photographs of the transit of the planet Venus across the Sun in 1874. This shows how the movies industry has evolved over the year to what it is now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.095444,
     "end_time": "2021-07-12T20:58:02.943139",
     "exception": false,
     "start_time": "2021-07-12T20:58:02.847695",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T20:58:03.142407Z",
     "iopub.status.busy": "2021-07-12T20:58:03.141685Z",
     "iopub.status.idle": "2021-07-12T20:58:03.156993Z",
     "shell.execute_reply": "2021-07-12T20:58:03.156390Z",
     "shell.execute_reply.started": "2021-07-12T20:42:01.596216Z"
    },
    "papermill": {
     "duration": 0.116516,
     "end_time": "2021-07-12T20:58:03.157161",
     "exception": false,
     "start_time": "2021-07-12T20:58:03.040645",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_imdb['runtime'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.095861,
     "end_time": "2021-07-12T20:58:03.349543",
     "exception": false,
     "start_time": "2021-07-12T20:58:03.253682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The average length of a movie is about 1 hour and 30 minutes. The longest movie on record in this dataset is a staggering 877 minutes (15 hours) long.\n",
    "\n",
    "We are aware that most movies(mainstream movies) are less than 5 hours (or 300 minutes) long.Below we plottin a distribution of these mainstream movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T20:58:03.548385Z",
     "iopub.status.busy": "2021-07-12T20:58:03.547592Z",
     "iopub.status.idle": "2021-07-12T20:58:03.991904Z",
     "shell.execute_reply": "2021-07-12T20:58:03.991290Z",
     "shell.execute_reply.started": "2021-07-12T20:44:10.580316Z"
    },
    "papermill": {
     "duration": 0.546568,
     "end_time": "2021-07-12T20:58:03.992076",
     "exception": false,
     "start_time": "2021-07-12T20:58:03.445508",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#plot showing distribution of mainstream movies\n",
    "df_imdb['runtime'] = df_imdb['runtime'].astype('float')\n",
    "plt.figure(figsize=(12,6))\n",
    "sns.distplot(df_imdb[(df_imdb['runtime'] < 300) & (df_imdb['runtime'] > 0)]['runtime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common Genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the df_movies data, which includes information on the genres each movie is categorised into, we can visualise which genres have the greatest number of movies. While this is not necessarily reflective of which genres are the most popular, it can give us an idea of what is contained within our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-06-14T11:20:08.543828Z",
     "iopub.status.busy": "2021-06-14T11:20:08.538723Z",
     "iopub.status.idle": "2021-06-14T11:20:08.772092Z",
     "shell.execute_reply": "2021-06-14T11:20:08.771517Z",
     "shell.execute_reply.started": "2021-06-06T09:44:14.282271Z"
    },
    "papermill": {
     "duration": 0.255771,
     "end_time": "2021-06-14T11:20:08.772264",
     "exception": false,
     "start_time": "2021-06-14T11:20:08.516493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dataframe containing only the movieId and genres\n",
    "movies_genres = pd.DataFrame(df_movies[['movieId', 'genres']],\n",
    "                             columns=['movieId', 'genres'])\n",
    "\n",
    "# Split genres seperated by \"|\" and create a list containing the genres allocated to each movie\n",
    "#movies_genres.genres = movies_genres.genres.apply(lambda x: x.split('|'))\n",
    "\n",
    "# Create expanded dataframe where each movie-genre combination is in a seperate row\n",
    "movies_genres = pd.DataFrame([(tup.movieId, d) for tup in movies_genres.itertuples() for d in tup.genres],\n",
    "                             columns=['movieId', 'genres'])\n",
    "\n",
    "movies_genres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T10:30:23.861808Z",
     "iopub.status.busy": "2021-07-12T10:30:23.861327Z",
     "iopub.status.idle": "2021-07-12T10:30:24.312409Z",
     "shell.execute_reply": "2021-07-12T10:30:24.311476Z",
     "shell.execute_reply.started": "2021-07-12T10:30:23.861779Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the genres from most common to least common\n",
    "plot = plt.figure(figsize=(15, 10))\n",
    "plt.title('Most common genres\\n', fontsize=20)\n",
    "sns.countplot(y=\"genres\", data=movies_genres,\n",
    "              order=movies_genres['genres'].value_counts(ascending=False).index,\n",
    "              palette='Reds_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph shows us that a majority of the movies in this database (over 25000!) are listed under the genre \"Drama\", and the least under the genre \"IMAX\". We can also see that there are quite a few movies, around 5000, that have no genres listed for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explore the distribution of ratings across the data, to see which ratings are given most often, and which least often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T10:30:55.033427Z",
     "iopub.status.busy": "2021-07-12T10:30:55.033038Z",
     "iopub.status.idle": "2021-07-12T10:30:55.089312Z",
     "shell.execute_reply": "2021-07-12T10:30:55.088162Z",
     "shell.execute_reply.started": "2021-07-12T10:30:55.033384Z"
    }
   },
   "outputs": [],
   "source": [
    "ratings = pd.DataFrame(df_train[['rating']],\n",
    "                             columns=['rating'])\n",
    "\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T10:30:55.544595Z",
     "iopub.status.busy": "2021-07-12T10:30:55.544244Z",
     "iopub.status.idle": "2021-07-12T10:30:56.902927Z",
     "shell.execute_reply": "2021-07-12T10:30:56.901855Z",
     "shell.execute_reply.started": "2021-07-12T10:30:55.544567Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.title('Ratings distribution \\n', fontsize=20)\n",
    "sns.countplot(x=\"rating\", data=ratings,\n",
    "              palette='Reds_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The countplot shows more movies are given ratings of 4.0 and 3.0, and fewer movies are given ratings of 0.5 and 1.5. This indicates that the movies in this dataset are often given average to good ratings, with fewer movies being rated poorly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings per Movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect of the data we can explore is how many ratings there are per movie. As there are many movies in the dataset, how many of these actually have ratings from a lot of users?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T10:31:24.193988Z",
     "iopub.status.busy": "2021-07-12T10:31:24.193621Z",
     "iopub.status.idle": "2021-07-12T10:31:24.494593Z",
     "shell.execute_reply": "2021-07-12T10:31:24.493620Z",
     "shell.execute_reply.started": "2021-07-12T10:31:24.193957Z"
    }
   },
   "outputs": [],
   "source": [
    "#Because there are so many movies in the database, we clip it at 100, otherwise the graph becomes too difficult to read\n",
    "ratings_per_movie = pd.DataFrame(df_train.groupby('movieId')['rating'].count().clip(upper=100))\n",
    "ratings_per_movie.groupby('rating')['rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T10:31:35.488792Z",
     "iopub.status.busy": "2021-07-12T10:31:35.488322Z",
     "iopub.status.idle": "2021-07-12T10:31:36.003044Z",
     "shell.execute_reply": "2021-07-12T10:31:36.002057Z",
     "shell.execute_reply.started": "2021-07-12T10:31:35.488752Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.title('Ratings per Movie \\n', fontsize=20)\n",
    "sns.histplot(data=ratings_per_movie,\n",
    "              palette='Reds_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows us that a vast majority of the movies (12537 movies) only have one rating. We can see that as the number of ratings per movie increases, the count of movies decreases, indicating that few movies have been rated by a large number of users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratings per User"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, we can also explore how many users are actually giving ratings to movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T10:31:58.962872Z",
     "iopub.status.busy": "2021-07-12T10:31:58.962512Z",
     "iopub.status.idle": "2021-07-12T10:31:59.346032Z",
     "shell.execute_reply": "2021-07-12T10:31:59.344978Z",
     "shell.execute_reply.started": "2021-07-12T10:31:58.962844Z"
    }
   },
   "outputs": [],
   "source": [
    "#Again, clipped at 500 because there are so many users, that including them all would make the graph difficult to read\n",
    "ratings_per_user = pd.DataFrame(df_train.groupby('userId')['rating'].count().clip(upper=500))\n",
    "ratings_per_user.groupby('rating')['rating'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T10:32:10.207979Z",
     "iopub.status.busy": "2021-07-12T10:32:10.207618Z",
     "iopub.status.idle": "2021-07-12T10:32:11.548798Z",
     "shell.execute_reply": "2021-07-12T10:32:11.547924Z",
     "shell.execute_reply.started": "2021-07-12T10:32:10.207945Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 10))\n",
    "plt.title('Ratings per User \\n', fontsize=20)\n",
    "sns.histplot(data=ratings_per_user,\n",
    "              palette='Reds_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the previous graph, we can see that the count of users dwindles as the number of ratings per user increases. Quite a lot of users appear to have given between 1 and 100 ratings, but increasingly fewer have given more ratings than this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.100865,
     "end_time": "2021-07-12T20:58:04.191840",
     "exception": false,
     "start_time": "2021-07-12T20:58:04.090975",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.098169,
     "end_time": "2021-07-12T20:58:04.388333",
     "exception": false,
     "start_time": "2021-07-12T20:58:04.290164",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Data-Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.097987,
     "end_time": "2021-07-12T20:58:05.293150",
     "exception": false,
     "start_time": "2021-07-12T20:58:05.195163",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Since keeping genres in a list format isn't optimal for the content-based recommendation system technique, we will use the One Hot Encoding technique to convert the list of genres to a vector where each column corresponds to one possible value of the feature. This encoding is needed for feeding categorical data. In this case, we store every different genre in columns that contain either 1 or 0. 1 shows that a movie has that genre and 0 shows that it doesn't. Let's also store this dataframe in another variable since genres won't be important for our first recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-12T20:58:05.493690Z",
     "iopub.status.busy": "2021-07-12T20:58:05.492947Z",
     "iopub.status.idle": "2021-07-12T20:58:05.619548Z",
     "shell.execute_reply": "2021-07-12T20:58:05.617974Z"
    },
    "papermill": {
     "duration": 0.228422,
     "end_time": "2021-07-12T20:58:05.619942",
     "exception": true,
     "start_time": "2021-07-12T20:58:05.391520",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Copying the movie dataframe into a new one since we won't need to use the genre information in our first case.\n",
    "moviesWithGenres_df = df_movies.copy()\n",
    "\n",
    "#For every row in the dataframe, iterate through the list of genres and place a 1 into the corresponding column\n",
    "for index, row in df_movies.iterrows():\n",
    "    for genre in row['genres']:\n",
    "        moviesWithGenres_df.at[index, genre] = 1\n",
    "#Filling in the NaN values with 0 to show that a movie doesn't have that column's genre\n",
    "moviesWithGenres_df = moviesWithGenres_df.fillna(0)\n",
    "moviesWithGenres_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now let's look at the ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.657353Z",
     "iopub.status.idle": "2021-07-12T13:53:57.658044Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Every row in the df_train dataframe has a user id associated with at least one movie. Here, we do not need the timestamp of when the users reviewed the movie. So, let's drop the 'timestamp' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.659282Z",
     "iopub.status.idle": "2021-07-12T13:53:57.659946Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train = df_train.drop('timestamp', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.66114Z",
     "iopub.status.idle": "2021-07-12T13:53:57.661771Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Content-Based Recomendation System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Let's begin by creating an input user to recommend movies to:\n",
    "\n",
    "Notice: To add more movies, simply increase the amount of elements in the userInput. Feel free to add more in! Just be sure to write it in with capital letters and if a movie starts with a \"The\", like \"The Matrix\" then write it in like this: 'Matrix, The' ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.663011Z",
     "iopub.status.idle": "2021-07-12T13:53:57.663641Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "userInput = [\n",
    "            {'title':'Breakfast Club, The', 'rating':5},\n",
    "            {'title':'Toy Story', 'rating':3.5},\n",
    "            {'title':'Jumanji', 'rating':2},\n",
    "            {'title':\"Pulp Fiction\", 'rating':5},\n",
    "            {'title':'Akira', 'rating':4.5}\n",
    "         ] \n",
    "inputMovies = pd.DataFrame(userInput)\n",
    "inputMovies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.664726Z",
     "iopub.status.idle": "2021-07-12T13:53:57.66519Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Filtering out the movies by title\n",
    "inputId = df_movies[df_movies['title'].isin(inputMovies['title'].tolist())]\n",
    "#Then merging it so we can get the movieId. It's implicitly merging it by title.\n",
    "inputMovies = pd.merge(inputId, inputMovies)\n",
    "#Dropping information we won't use from the input dataframe\n",
    "inputMovies = inputMovies.drop('genres', 1).drop('year', 1)\n",
    "#Final input dataframe\n",
    "#If a movie you added in above isn't here, then it might not be in the original \n",
    "#dataframe or it might spelled differently, please check capitalisation.\n",
    "inputMovies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.666071Z",
     "iopub.status.idle": "2021-07-12T13:53:57.666465Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Filtering out the movies from the input\n",
    "userMovies = moviesWithGenres_df[moviesWithGenres_df['movieId'].isin(inputMovies['movieId'].tolist())]\n",
    "userMovies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We'll only need the actual genre table, so let's clean this up a bit by resetting the index and dropping the movieId, title, genres and year columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.66734Z",
     "iopub.status.idle": "2021-07-12T13:53:57.667728Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Resetting the index to avoid future issues\n",
    "userMovies = userMovies.reset_index(drop=True)\n",
    "#Dropping unnecessary issues due to save memory and to avoid issues\n",
    "userGenreTable = userMovies.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)\n",
    "userGenreTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now we're ready to start learning the input's preferences!\n",
    "\n",
    "To do this, we're going to turn each genre into weights. We can do this by using the input's reviews and multiplying them into the input's genre table and then summing up the resulting table by column. This operation is actually a dot product between a matrix and a vector, so we can simply accomplish by calling Pandas's \"dot\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.668489Z",
     "iopub.status.idle": "2021-07-12T13:53:57.668941Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "inputMovies['rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.669824Z",
     "iopub.status.idle": "2021-07-12T13:53:57.670223Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dot produt to get weights\n",
    "userProfile = userGenreTable.transpose().dot(inputMovies['rating'])\n",
    "#The user profile\n",
    "userProfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now, we have the weights for every of the user's preferences. This is known as the User Profile. Using this, we can recommend movies that satisfy the user's preferences.\n",
    "\n",
    "Let's start by extracting the genre table from the original dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.671132Z",
     "iopub.status.idle": "2021-07-12T13:53:57.67152Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Now let's get the genres of every movie in our original dataframe\n",
    "genreTable = moviesWithGenres_df.set_index(moviesWithGenres_df['movieId'])\n",
    "#And drop the unnecessary information\n",
    "genreTable = genreTable.drop('movieId', 1).drop('title', 1).drop('genres', 1).drop('year', 1)\n",
    "genreTable.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.672519Z",
     "iopub.status.idle": "2021-07-12T13:53:57.672958Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "genreTable.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "With the input's profile and the complete list of movies and their genres in hand, we're going to take the weighted average of every movie based on the input profile and recommend the top twenty movies that most satisfy it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.673811Z",
     "iopub.status.idle": "2021-07-12T13:53:57.674244Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Multiply the genres by the weights and then take the weighted average\n",
    "recommendationTable_df = ((genreTable*userProfile).sum(axis=1))/(userProfile.sum())\n",
    "recommendationTable_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.675011Z",
     "iopub.status.idle": "2021-07-12T13:53:57.675406Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sort our recommendations in descending order\n",
    "recommendationTable_df = recommendationTable_df.sort_values(ascending=False)\n",
    "#Just a peek at the values\n",
    "recommendationTable_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now here's the recommendation table!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.676244Z",
     "iopub.status.idle": "2021-07-12T13:53:57.676634Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#The final recommendation table\n",
    "df_movies.loc[df_movies['movieId'].isin(recommendationTable_df.head(20).keys())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Although content-based recommender systems are highly personalized for the user and they learn the user's preferences, one major disadvantage of them is that they don't take into account of what others may think of the item. So low quality recommendations may happen. This is where collaborative filtering steps in to resolve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Let's also drop the genres column since we won't need it for this particular recommendation system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.677463Z",
     "iopub.status.idle": "2021-07-12T13:53:57.677866Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Dropping the genres column\n",
    "df_movies = df_movies.drop('genres', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.67873Z",
     "iopub.status.idle": "2021-07-12T13:53:57.679181Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now, time to start our work on recommendation systems. \n",
    "\n",
    "The first technique we're going to take a look at is called __Collaborative Filtering__, which is also known as __User-User Filtering__. As hinted by its alternate name, this technique uses other users to recommend items to the input user. It attempts to find users that have similar preferences and opinions as the input and then recommends items that they have liked to the input. There are several methods of finding similar users (Even some making use of Machine Learning), and the one we will be using here is going to be based on the __Pearson Correlation Function__.\n",
    "\n",
    "<img src=\"https://ibm.box.com/shared/static/1ql8cbwhtkmbr6nge5e706ikzm5mua5w.png\" width=800px>\n",
    "\n",
    "\n",
    "The process for creating a User Based recommendation system is as follows:\n",
    "- Select a user with the movies the user has watched\n",
    "- Based on his rating to movies, find the top X neighbours \n",
    "- Get the watched movie record of the user for each neighbour.\n",
    "- Calculate a similarity score using some formula\n",
    "- Recommend the items with the highest score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### The users who has seen the same movies\n",
    "Now with the movie ID's in our input, we can now get the subset of users that have watched and reviewed the movies in our input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.680101Z",
     "iopub.status.idle": "2021-07-12T13:53:57.680493Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Filtering out users that have watched movies that the input has watched and storing it\n",
    "userSubset = df_train[df_train['movieId'].isin(inputMovies['movieId'].tolist())]\n",
    "userSubset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We now group up the rows by user ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.681484Z",
     "iopub.status.idle": "2021-07-12T13:53:57.681889Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Groupby creates several sub dataframes where they all have the same value in the column specified as the parameter\n",
    "userSubsetGroup = userSubset.groupby(['userId'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Let's look at one of the users, say the one with a userId == 97088."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.6828Z",
     "iopub.status.idle": "2021-07-12T13:53:57.683199Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "userSubsetGroup.get_group(97088)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Let's also sort these groups so the users that share the most movies in common with the input have higher priority. This provides a richer recommendation since we won't go through every single user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.684026Z",
     "iopub.status.idle": "2021-07-12T13:53:57.684423Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Sorting it so users with movie most in common with the input will have priority\n",
    "userSubsetGroup = sorted(userSubsetGroup,  key=lambda x: len(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Let's look at the first 3 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.685355Z",
     "iopub.status.idle": "2021-07-12T13:53:57.685762Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "userSubsetGroup[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Similarity of users to input user\n",
    "Next, we are going to compare all users (not really all !!!) to our specified user and find the one that is most similar.  \n",
    "we're going to find out how similar each user is to the input through the __Pearson Correlation Coefficient__. It is used to measure the strength of a linear association between two variables. The formula for finding this coefficient between sets X and Y with N values can be seen in the image below. \n",
    "\n",
    "Why Pearson Correlation?\n",
    "\n",
    "Pearson correlation is invariant to scaling, i.e. multiplying all elements by a nonzero constant or adding any constant to all elements. For example, if you have two vectors X and Y,then, pearson(X, Y) == pearson(X, 2 * Y + 3). This is a pretty important property in recommendation systems because for example two users might rate two series of items totally different in terms of absolute rates, but they would be similar users (i.e. with similar ideas) with similar rates in various scales .\n",
    "\n",
    "![alt text](https://wikimedia.org/api/rest_v1/media/math/render/svg/bd1ccc2979b0fd1c1aec96e386f686ae874f9ec0 \"Pearson Correlation\")\n",
    "\n",
    "The values given by the formula vary from r = -1 to r = 1, where 1 forms a direct correlation between the two entities (it means a perfect positive correlation) and -1 forms a perfect negative correlation. \n",
    "\n",
    "In our case, a 1 means that the two users have similar tastes while a -1 means the opposite."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We will select a subset of users to iterate through. This limit is imposed because we don't want to waste too much time going through every single user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.686805Z",
     "iopub.status.idle": "2021-07-12T13:53:57.687225Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "userSubsetGroup = userSubsetGroup[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now, we calculate the Pearson Correlation between input user and subset group, and store it in a dictionary, where the key is the user Id and the value is the coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.687925Z",
     "iopub.status.idle": "2021-07-12T13:53:57.688328Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.68912Z",
     "iopub.status.idle": "2021-07-12T13:53:57.689506Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Store the Pearson Correlation in a dictionary, where the key is the user Id and the value is the coefficient\n",
    "pearsonCorrelationDict = {}\n",
    "\n",
    "#For every user group in our subset\n",
    "for name, group in userSubsetGroup:\n",
    "    #Let's start by sorting the input and current user group so the values aren't mixed up later on\n",
    "    group = group.sort_values(by='movieId')\n",
    "    inputMovies = inputMovies.sort_values(by='movieId')\n",
    "    #Get the N for the formula\n",
    "    nRatings = len(group)\n",
    "    #Get the review scores for the movies that they both have in common\n",
    "    temp_df = inputMovies[inputMovies['movieId'].isin(group['movieId'].tolist())]\n",
    "    #And then store them in a temporary buffer variable in a list format to facilitate future calculations\n",
    "    tempRatingList = temp_df['rating'].tolist()\n",
    "    #Let's also put the current user group reviews in a list format\n",
    "    tempGroupList = group['rating'].tolist()\n",
    "    #Now let's calculate the pearson correlation between two users, so called, x and y\n",
    "    Sxx = sum([i**2 for i in tempRatingList]) - pow(sum(tempRatingList),2)/float(nRatings)\n",
    "    Syy = sum([i**2 for i in tempGroupList]) - pow(sum(tempGroupList),2)/float(nRatings)\n",
    "    Sxy = sum( i*j for i, j in zip(tempRatingList, tempGroupList)) - sum(tempRatingList)*sum(tempGroupList)/float(nRatings)\n",
    "    \n",
    "    #If the denominator is different than zero, then divide, else, 0 correlation.\n",
    "    if Sxx != 0 and Syy != 0:\n",
    "        pearsonCorrelationDict[name] = Sxy/sqrt(Sxx*Syy)\n",
    "    else:\n",
    "        pearsonCorrelationDict[name] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.690205Z",
     "iopub.status.idle": "2021-07-12T13:53:57.690587Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pearsonCorrelationDict.items()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "As we can see, this dictionary is an eyesore and it is better to have the values stored in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.691617Z",
     "iopub.status.idle": "2021-07-12T13:53:57.692055Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pearsonDF = pd.DataFrame.from_dict(pearsonCorrelationDict, orient='index')\n",
    "pearsonDF.columns = ['similarityIndex']\n",
    "pearsonDF['userId'] = pearsonDF.index\n",
    "pearsonDF.index = range(len(pearsonDF))\n",
    "pearsonDF.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### The top x similar users to input user\n",
    "Now let's get the top 50 users that are most similar to the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.692904Z",
     "iopub.status.idle": "2021-07-12T13:53:57.693307Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topUsers=pearsonDF.sort_values(by='similarityIndex', ascending=False)[0:50]\n",
    "topUsers.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Great, now then let's start recommending some movies to the input user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "#### Rating of selected users to all movies\n",
    "We're going to do this by taking the weighted average of the ratings of the movies using the Pearson Correlation as the weight. But to do this, we first need to get the movies watched by the users in our __pearsonDF__ from the train dataframe and then store their correlation in a new column called _similarityIndex\". This is achieved below by merging of these two tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.694207Z",
     "iopub.status.idle": "2021-07-12T13:53:57.694608Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "topUsersRating=topUsers.merge(df_train, left_on='userId', right_on='userId', how='inner')\n",
    "topUsersRating.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now all we need to do is simply multiply the movie rating by its weight (The similarity index), then sum up the new ratings and divide it by the sum of the weights.\n",
    "\n",
    "We can easily do this by simply multiplying two columns, then grouping up the dataframe by movieId and then dividing two columns:\n",
    "\n",
    "It shows the idea of all similar users to candidate movies for the input user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.695392Z",
     "iopub.status.idle": "2021-07-12T13:53:57.695805Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Multiplies the similarity by the user's ratings\n",
    "topUsersRating['weightedRating'] = topUsersRating['similarityIndex']*topUsersRating['rating']\n",
    "topUsersRating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.696531Z",
     "iopub.status.idle": "2021-07-12T13:53:57.696967Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Applies a sum to the topUsers after grouping it up by userId\n",
    "tempTopUsersRating = topUsersRating.groupby('movieId').sum()[['similarityIndex','weightedRating']]\n",
    "tempTopUsersRating.columns = ['sum_similarityIndex','sum_weightedRating']\n",
    "tempTopUsersRating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.697718Z",
     "iopub.status.idle": "2021-07-12T13:53:57.698129Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creates an empty dataframe\n",
    "recommendation_df = pd.DataFrame()\n",
    "#Now we take the weighted average\n",
    "recommendation_df['weighted average recommendation score'] = tempTopUsersRating['sum_weightedRating']/tempTopUsersRating['sum_similarityIndex']\n",
    "recommendation_df['movieId'] = tempTopUsersRating.index\n",
    "recommendation_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now let's sort the recommendation dataframe and see just the top 20 movies that the algorithm recommended. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.699054Z",
     "iopub.status.idle": "2021-07-12T13:53:57.699453Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "recommendation_df = recommendation_df.sort_values(by='weighted average recommendation score', ascending=False)\n",
    "recommendation_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We need to make the recommended movies more recognizable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.700384Z",
     "iopub.status.idle": "2021-07-12T13:53:57.700874Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_movies.loc[df_movies['movieId'].isin(recommendation_df.head(10)['movieId'].tolist())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.701776Z",
     "iopub.status.idle": "2021-07-12T13:53:57.702173Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from surprise import Reader, Dataset\n",
    "reader = Reader()\n",
    "data = Dataset.load_from_df(df_train[['userId', 'movieId', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.70316Z",
     "iopub.status.idle": "2021-07-12T13:53:57.703558Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Singular Value Decomposition (SVD), a method from linear algebra that has been generally used as a dimensionality reduction technique in machine learning. SVD is a matrix factorisation technique, which reduces the number of features of a dataset by reducing the space dimension from N-dimension to K-dimension (where K<N). In the context of the recommender system, the SVD is used as a collaborative filtering technique. It uses a matrix structure where each row represents a user, and each column represents an item. The elements of this matrix are the ratings that are given to items by users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.704348Z",
     "iopub.status.idle": "2021-07-12T13:53:57.704758Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from surprise import SVD, accuracy\n",
    "algo = SVD()\n",
    "algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.705479Z",
     "iopub.status.idle": "2021-07-12T13:53:57.70589Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions = algo.test(testset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.706783Z",
     "iopub.status.idle": "2021-07-12T13:53:57.707178Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from surprise import SVD,Reader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.70801Z",
     "iopub.status.idle": "2021-07-12T13:53:57.708412Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Load Reader library\n",
    "# reader = Reader()\n",
    "\n",
    "# # Load ratings dataset with Dataset library\n",
    "# data = Dataset.load_from_df(df_train[['userId', 'movieId', 'rating']], reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.709181Z",
     "iopub.status.idle": "2021-07-12T13:53:57.709581Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the SVD algorithm.\n",
    "svd = SVD()\n",
    "\n",
    "# Compute the RMSE of the SVD algorithm.\n",
    "#cross_evaluate(svd, data, measures=['RMSE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.710438Z",
     "iopub.status.idle": "2021-07-12T13:53:57.710844Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# cross_validate(svd, data, measures=['RMSE'], cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "We get a mean Root Mean Square Error of 0.85 which is pretty good. Let's now train on the dataset and arrive at predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.711774Z",
     "iopub.status.idle": "2021-07-12T13:53:57.712187Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# trainset = data.build_full_trainset()\n",
    "# svd.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Now let's use SVD to predict the rating that, for instance, a User with ID 150 will give to a random movie (let's say with Movie ID 1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.713308Z",
     "iopub.status.idle": "2021-07-12T13:53:57.713731Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# svd.predict(150, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "For a movie with movieId 1000, User with ID 150 is predicted to give a rating of 3.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### SVD++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.714897Z",
     "iopub.status.idle": "2021-07-12T13:53:57.71537Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from surprise.model_selection import train_test_split\n",
    "\n",
    "# trainset, testset = train_test_split(data, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.71624Z",
     "iopub.status.idle": "2021-07-12T13:53:57.716655Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# algo_svdpp = SVDpp(n_factors=160, n_epochs=10, lr_all=0.005, reg_all=0.1)\n",
    "# algo_svdpp.fit(trainset)\n",
    "# test_pred = algo_svdpp.test(testset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Baseline Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A baseline is a method that uses heuristics, simple summary statistics, randomness, or machine learning to create predictions for a dataset.The purpose of using a baseline when evaluating a machine learning model algorithm is to provides a set of predictions that you can evaluate as you would any predictions for your problem, such as classification accuracy or RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.717616Z",
     "iopub.status.idle": "2021-07-12T13:53:57.71805Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bsl_options = {'method': 'sgd','n_epochs': 35}\n",
    "blo = BaselineOnly(bsl_options=bsl_options)\n",
    "blo.fit(trainset)\n",
    "predictions = blo.test(testset)\n",
    "blo_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### SlopeOne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slope one algorithm is a special form of Item-based collaborative filtering algorithm, which has the characteristics of easy to use, accurate recommendation and high computational efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.719108Z",
     "iopub.status.idle": "2021-07-12T13:53:57.719522Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SlopeOne\n",
    "slo_test = SlopeOne()\n",
    "slo_test.fit(trainset)\n",
    "predictions = slo_test.test(testset)\n",
    "# Calculate RMSE\n",
    "slo_rmse = accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Non-Negative Matrix Factorization (NMF) is an unsupervised algorithm that projects data into lower dimensional spaces, effectively reducing the number of features while retaining the basis information necessary to reconstruct the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.720546Z",
     "iopub.status.idle": "2021-07-12T13:53:57.721011Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nmf = NMF()\n",
    "nmf.fit(trainset)\n",
    "nmf_p = nmf.test(testset)\n",
    "nmf_rmse = accuracy.rmse(nmf_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Normal Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NormalPredictor algorithm predicts a random rating based on the distribution of the training set, which is assumed to be normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.722Z",
     "iopub.status.idle": "2021-07-12T13:53:57.722433Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "np = NormalPredictor()\n",
    "np.fit(trainset)\n",
    "np_p = np.test(testset)\n",
    "np_rmse = accuracy.rmse(np_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### CoClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Co-clustering simultaneously cluster both data points and features of a two-dimensional data matrix. It is a powerful data analysis technique that can discover latent patterns hidden within particular rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-07-12T13:53:57.723299Z",
     "iopub.status.idle": "2021-07-12T13:53:57.723699Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cc = CoClustering(random_state=42)\n",
    "cc.fit(trainset)\n",
    "cc_p = cc.test(testset)\n",
    "cc_rmse = accuracy.rmse(cc_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We built and tested different types of collaborative filtering models . Their performance was compared using a statistical measure known as the root mean squared error (RMSE), which determines the average squared difference between the estimated values and the actual value. A low RMSE value indicates high model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Root Mean Squared Error (RMSE):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "$$RMSE = \\sqrt{\\frac{1}{|\\hat{R}|}\\Sigma_{r_{ui} \\in {\\hat{R}}}{\\Big({r_{ui} -{\\hat {r}}_{ui}}\\Big)^2}}$$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "Where     is the total number of recommendations generated for users and movies, with    and     being the true and predicted ratings for user u watching movie i respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Compare RMSE Between Models\n",
    "fig,axis = plt.subplots(figsize=(14, 5))\n",
    "models = ['SVD','Normal Predictor','NMF','CoClustering','Baseline Only','SlopeOne']\n",
    "rmse_y = [svd_rmse,np_rmse,nmf_rmse,cc_rmse,blo_rmse,slo_rmse]\n",
    "ax = sns.barplot(x=models, y=rmse_y,palette='winter')\n",
    "plt.title('RMSE Value Per Collaborative-based Filtering Model',fontsize=14)\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('RMSE')\n",
    "for p in ax.patches:\n",
    "    ax.text(p.get_x() + p.get_width()/2, p.get_y() + p.get_height(), round(p.get_height(),4), fontsize=12, ha=\"center\", va='bottom')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# test predictions\n",
    "predictions = []\n",
    "for i, row in df_test.iterrows():\n",
    "    x = (algo.predict(row.userId, row.movieId))\n",
    "    pred = x[3]\n",
    "    predictions.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['Id'] =df_test['userId'].map(str) +'_'+ df_test['movieId'].map(str)\n",
    "results = pd.DataFrame({\"Id\":df_test['Id'],\"rating\": predictions})\n",
    "results.to_csv(\"Team JS1-Unsupervised.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 72.79455,
   "end_time": "2021-07-12T20:58:06.831613",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-07-12T20:56:54.037063",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
