{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Unsupervised Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This notebook has been prepared by:**\n",
    "* Zizipho Tyeko\n",
    "* Siyamanga Malawu\n",
    "* Lejone Malokosta\n",
    "* Pfano Phungo\n",
    "* Mogau Mogashoa\n",
    "* Dunyiswa Matshaya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is the notebook going to work?\n",
    "\n",
    "This notebook is a layout of a recommender system that is used to predict a movie user possible rating. The notebook will make use of the recommender system methods and techniques using sequential steps to get to the prediction of the possible expected results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Movie Recommendation Challenge**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Recommender System**\n",
    "\n",
    "Recommender systems are amid the most well known applications of data science today. They are used to predict the \"rating\" or \"preference\" that a user would possibly give to an item. Recommender systems uses its techniques by searching through large volume of dynamically generated information to provide users with personalized content and services.\n",
    "Technically recommender system has the ability to predict whether a particular user would prefer an item or not based on the user’s profile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to predict how a user will rate a movie they have not yet viewed, based on their historical preference on a movie website or application e.g Netflix, Showmax or Amazon Prime.\n",
    "\n",
    "Movie websites and applications can improve their reliability and enhance their customer experience by providing an estimated rating or preference of a movie through a recommender system used to model the predicted results.\n",
    "\n",
    "Recommender systems are essential economically and socially in today's technology driven world. This can help movie companies in ensuring that their users can make the appropriate choices surrounding the content that they regulary engage with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Aim**\n",
    "Design a recommendersystem which will predict a user possible rating on a movie that they have not viewed yet based on they user history of their movie ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Scope**\n",
    "The scope of this project is to analyse and search through large volume of dynamically generated information consisting of movie ratings given by a user and information describing the movie.\n",
    "These ratings will be used to train machine learning models to help with the prediction of the ratings given by a user on an unseen movie. This could also help with providing users with personalised content and services."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://posteet.com/wp-content/uploads/2019/11/movies.png\" width=90%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# **Table of Content**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "1. Import packages\n",
    "2. Loading Datasets\n",
    "3. Data Description\n",
    "4. Explanotory Data Analysis\n",
    "5. Data Filtering\n",
    "6. Varibale Selection\n",
    "7. Modeling\n",
    "8. Model Comparison\n",
    "9. Model Explanation\n",
    "10. Submission\n",
    "11. Application Pickled files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install comet_ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comet_ml in the top of your file\n",
    "import comet_ml\n",
    "from comet_ml import Experiment\n",
    "\n",
    "experiment = Experiment(api_key=\"PXaqoLTlHzWF85kimEv0zHJPz\",\n",
    "                        project_name=\"unsupervised-predict\", workspace=\"lejone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilities\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#pre-processing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "#plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import seaborn as sns\n",
    "import cufflinks as cf\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "sns.set()\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel, cosine_similarity\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from surprise import Reader, Dataset, SVD, SVDpp\n",
    "from surprise.model_selection import cross_validate\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from datetime import datetime\n",
    "import datetime\n",
    "from timeit import default_timer\n",
    "\n",
    "#others\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context manager allows logging og parameters\n",
    "\n",
    "experiment.context_manager(\"validate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL where experiments can be found\n",
    "\n",
    "experiment.url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Loading Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section loads and checks the data needed to run this notebook.\n",
    "To ensure the notebook runs without any error, the relevent csv files need to be present in the notebook's directory before running the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/edsa-recommender-system-predict/train.csv') \n",
    "test = pd.read_csv('../input/edsa-recommender-system-predict/test.csv')\n",
    "scores = pd.read_csv('../input/edsa-recommender-system-predict/genome_scores.csv')\n",
    "tags = pd.read_csv('../input/edsa-recommender-system-predict/genome_tags.csv')\n",
    "imdb = pd.read_csv('../input/edsa-recommender-system-predict/imdb_data.csv') \n",
    "links = pd.read_csv('../input/edsa-recommender-system-predict/links.csv') \n",
    "movies = pd.read_csv('../input/edsa-recommender-system-predict/movies.csv')\n",
    "sample = pd.read_csv('../input/edsa-recommender-system-predict/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train\\n\")\n",
    "print(train.head(5))\n",
    "print(\"Test\\n\")\n",
    "print(test.head(5))\n",
    "print(\"Scores\\n\")\n",
    "print(scores.head(5))\n",
    "print(\"Tags\\n\")\n",
    "print(tags.head(5))\n",
    "print(\"imdb\\n\")\n",
    "print(imdb.head(5))\n",
    "print(\"Links\\n\")\n",
    "print(links.head(5))\n",
    "print(\"\\nMovies\")\n",
    "print(movies.head(5))\n",
    "\n",
    "print(\"\\n(Datasets were imported correctly)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have loaded the datasets into the notebook here we display the information of the datasets that we will be using, check if there are gaps or blanks in the datasets and check how the sentyiments of the rating look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Checking the data types of the train data sets and the unique values of the rating column in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train.info())\n",
    "print('The rating sentiments are as follows: /n')\n",
    "print(train['rating'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset consist of the user rating information, which is the user ID, the movie the user rated using the movie ID, the timestamp that the rating was made by the user and the rating itself. \n",
    "The rating sentiment ranges from 0.5 to 5. With 0.5 being the lowest rating and 5 being the highest rating of a movie. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Checking and removing null values from the datasets that we are planning to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of Nan values in our dataframe : \", sum(train.isnull().any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of Nan values in our dataframe : \", sum(test.isnull().any()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"No of Nan values in our dataframe : \", sum(movies.isnull().any()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above datasets are the ones that we will be using the most in this notebook. So we are checking if there is any null values that we would need to remove from the datasets. However reading from the results above there are no null values from the datasets at all. So because of that we won't need to remove any null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Checking and removing duplicate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate entries for userId, movieId and rating in the train dataset \n",
    "dup_bool = train.duplicated(['userId','movieId','rating'])\n",
    "dups = sum(dup_bool) \n",
    "print(\"There are {} duplicate rating entries in the data..\".format(dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate entries for userId, movieId in the test dataset \n",
    "dup_bool = test.duplicated(['userId','movieId'])\n",
    "dups = sum(dup_bool) \n",
    "print(\"There are {} duplicate rating entries in the data..\".format(dups))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate entries for movieId, title, genres in the movies dataset \n",
    "dup_bool = movies.duplicated(['movieId', 'title', 'genres'])\n",
    "dups = sum(dup_bool) \n",
    "print(\"There are {} duplicate rating entries in the data..\".format(dups))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also checking for any duplicated data in the above datasets. As we we can see we have 0 duplicated data, so no removal of any duplicates from the datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Calculating the number of user's, movies and ratings from the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''box plot for ratings'''\n",
    "box = train['rating']\n",
    "plt.boxplot(box)\n",
    "plt.ylabel('Ratings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above box plot is illustrating the 5 number summary of the ratings in the train dataset. 75% of the ratings are above 3 this means that people like the movies on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total data \")\n",
    "print(\"-\"*50)\n",
    "print(\"\\nTotal no of ratings :\",train.shape[0])\n",
    "print(\"Total No of Users   :\", len(np.unique(train.userId)))\n",
    "print(\"Total No of movies  :\", len(np.unique(train.movieId)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code is showing us the total length of the features in the train dataset.\n",
    "This is the total number of the ratings, the total number of users and the total number of movies rated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will use a method called \"human\". This method will just help us with making one of our graphs more readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''This is a method that improves the readability of a graph.'''\n",
    "def human(num, units = 'M'):\n",
    "    units = units.lower()\n",
    "    num = float(num)\n",
    "    if units == 'k':\n",
    "        return str(num/10**3) + \" K\"\n",
    "    elif units == 'm':\n",
    "        return str(num/10**6) + \" M\"\n",
    "    elif units == 'b':\n",
    "        return str(num/10**9) +  \" B\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.title('Distribution of ratings over Training dataset', fontsize=15)\n",
    "sns.countplot(train.rating)\n",
    "ax.set_yticklabels([human(item, 'M') for item in ax.get_yticks()])\n",
    "ax.set_ylabel('No. of Ratings(Millions)')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph we can see that we have over 2.5 million 4.0 scale number of ratings follwed by the 3.0 scale that has a little below 2 million number of ratings and in 3rd place it is the 5.0 scale that has a little below 1.5 million ratings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Analysis of ratings of a movie given by a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To analyse the ratings given by a user for a specific movie, we will use the movieId and rating features from the Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_ratings_per_movie = train_df.groupby(by='movieId')['rating'].count().sort_values(ascending=False)\n",
    "\n",
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "ax = plt.gca()\n",
    "plt.plot(no_of_ratings_per_movie.values)\n",
    "plt.title('Number of ratings per Movie')\n",
    "plt.xlabel('Movie')\n",
    "plt.ylabel('No of Users who rated a movie')\n",
    "ax.set_xticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot is very skewed.. just like number of ratings given per user.\n",
    "\n",
    "There are some movies which are very popular which are rated by a huge number of users, though they are very few (less than 15 %).\n",
    "\n",
    "But most of the movies(about 85%) have ratings below 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Converting timestamp column into date time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we are converting the timestamp feature in the T\n",
    "rain dataset from an integer datatype to a datetime datatype so that we can see the actual date of the rating, the actual month, the day of the week and the year that the rating was given by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''converting timestamp column to datetime'''\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'], unit='ms')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3.1 Adding a day of week column in the train data set to analyse ratings on each day of the week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''extracting days of the week'''\n",
    "train_df['day_of_week'] = train_df['timestamp'].dt.dayofweek\n",
    "days = {0:'Mon',1:'Tues',2:'Weds',3:'Thurs',4:'Fri',5:'Sat',6:'Sun'}\n",
    "train_df['day_of_week'] = train_df['day_of_week'].apply(lambda x: days[x])\n",
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plotting average number of ratings for each day'''\n",
    "train_df['day_of_week'].value_counts().plot(kind='barh')\n",
    "plt.xlabel('Average number of ratings')\n",
    "plt.ylabel('Days of the week')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calcated the average rating for each day of the week and we can see that we receive the most ratings on a Sunday and on a Saturday. As that would be the time that user's are more settled at home, because it would be end of week and thats the time that most user's would have time to watch movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Analysis of ratings given by a user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we will analyse the ratings that are made by the users. We use the userId and the rating feature from the Train dataset to do this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_rated_movies_per_user = train_df.groupby(by='userId')['rating'].count().sort_values(ascending=False)\n",
    "\n",
    "no_of_rated_movies_per_user.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see the total number of ratings of the top 5 users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''plotting the PDF and CDF graphs'''\n",
    "fig = plt.figure(figsize=plt.figaspect(.5))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.kdeplot(no_of_rated_movies_per_user, shade=True, ax=ax1)\n",
    "plt.xlabel('No of ratings by user')\n",
    "plt.title(\"PDF\")\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.kdeplot(no_of_rated_movies_per_user, shade=True, cumulative=True,ax=ax2)\n",
    "plt.xlabel('No of ratings by user')\n",
    "plt.title('CDF')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most users seem to have low ratings. This is evident from the PDF plot above as number of ratings by user is very low after 2000 ratings. There also seems to be some more users with ratings around 10 000 and less. We now have to investigate the distribution of the ratings further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_rated_movies_per_user.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles = no_of_rated_movies_per_user.quantile(np.arange(0,1.01,0.01), interpolation='higher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Quantiles and their Values\")\n",
    "quantiles.plot()\n",
    "'''quantiles with 0.05 difference'''\n",
    "plt.scatter(x=quantiles.index[::5], y=quantiles.values[::5], c='orange', label=\"quantiles with 0.05 intervals\")\n",
    "# quantiles with 0.25 difference\n",
    "plt.scatter(x=quantiles.index[::25], y=quantiles.values[::25], c='m', label = \"quantiles with 0.25 intervals\")\n",
    "plt.ylabel('No of ratings by user')\n",
    "plt.xlabel('Value at the quantile')\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# annotate the 25th, 50th, 75th and 100th percentile values....\n",
    "for x,y in zip(quantiles.index[::25], quantiles[::25]):\n",
    "    plt.annotate(s=\"({} , {})\".format(x,y), xy=(x,y), xytext=(x-0.05, y+500)\n",
    "                ,fontweight='bold')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we see that most user ratings are higher on the 75th percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantiles[::5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many ratings do we have at the last 5% of all ratings??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n No of ratings at last 5 percentile : {}\\n'.format(sum(no_of_rated_movies_per_user>= 222)) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per our observation earlier, number of ratings at last 5 percentile is 8182 out of 162541 ratings. The mean ratings is 49.218536 with standard deviation of 86.009691."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Calculating the number of movies per genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which genre appears the most with the rated movies?\n",
    "\n",
    "Below we will look at the genres feature from the Movies dataset and see which genre seems to appear the most with the rated movies. \n",
    "\n",
    "We made use of the count_genre function that will help us count the number of times each genre is mentioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Split the genres into individual genres'''\n",
    "genre_labels = set()\n",
    "for s in movies_df['genres'].str.split('|').values:\n",
    "    genre_labels = genre_labels.union(set(s))\n",
    "    \n",
    "'''Function that counts the number of times each of the genre appear'''\n",
    "def count_genre(dataset, ref_col, census):\n",
    "    genre_count = dict()\n",
    "    for s in census: \n",
    "        genre_count[s] = 0\n",
    "    for census_keywords in dataset[ref_col].str.split('|'):        \n",
    "        if type(census_keywords) == float and pd.isnull(census_keywords): \n",
    "            continue        \n",
    "        for s in [s for s in census_keywords if s in census]: \n",
    "            if pd.notnull(s): \n",
    "                genre_count[s] += 1\n",
    "    '''______________________________________________________________________\n",
    "     convert the dictionary in a list to sort the keywords by frequency '''\n",
    "    genre_freq = []\n",
    "    for k,v in genre_count.items():\n",
    "        genre_freq.append([k,v])\n",
    "    genre_freq.sort(key = lambda x:x[1], reverse = True)\n",
    "    return genre_freq, genre_count\n",
    "\n",
    "'''Calling this function gives access to a list of genre keywords which are sorted by decreasing frequency'''\n",
    "genre_freq, dum = count_genre(movies_df, 'genres', genre_labels)\n",
    "genre_freq[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Convert the frequency to dictionary'''\n",
    "genres = dict()\n",
    "trunc_freq = genre_freq[0:18]\n",
    "for s in trunc_freq:\n",
    "    genres[s[0]] = s[1]\n",
    "    \n",
    "    '''Convert to Dataframe'''\n",
    "df=pd.DataFrame.from_dict(genres, orient='index').transpose()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_list = list(genres.keys())\n",
    "\n",
    "'''Plot a bar plot of genres sorted in ascending order'''\n",
    "plt.figure(figsize=(12,7))\n",
    "genre_counts = df.loc[:,genres_list].sum().sort_values(ascending=False)\n",
    "sns.barplot(x=genre_counts.index, y=genre_counts.values)\n",
    "plt.xticks(rotation=60)\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Number of movies');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above histogram we can see that the Drama genre seems to be having the most movies tags with a little over 25000 released movies followed by the Comedy genre that has over over 15000 movies. This could possibly mean that most ratings are coming from these two genres as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 Using WordCloud to check which words appear most frequent under genres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want use the WordCloud technique to read into our genres to see which words is commonly found in each genre of a movie. \n",
    "This will help us confirm the above graph as well, because if a movie genre appears the most then that will mean that it is tagged the most in the rated movies.\n",
    "We will make use of the Movies and the Tags datasets as it will help us analyse the the data we need using the genres feature from the Movies dataset and the tag feature from the Tag dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies['genres'] = movies['genres'].astype('str')\n",
    "tags['tag'] = tags['tag'].astype('str')\n",
    "overview_corpus = ' '.join(movies['genres'])\n",
    "tag = ' '.join(tags['tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview_wordcloud = WordCloud(stopwords=STOPWORDS, background_color='white', height=2000, width=4000).generate(overview_corpus)\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.imshow(overview_wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wordcloud confirms the genre graph above where Drama and Comedy seems to appear to have the most movies. The wordcloud shows that the Drama and Comedy words appear the most or are most common on the movie genres and seems to be tagged more than other genres as well. These two genres are also followed by the Romance and Thriller genres that also seem to be common words on the WordCloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6 Calculating the number of movies released each year and number of ratings each year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can run any graphs we first need to do the following:\n",
    "\n",
    "* Split title and year into separate columns. Convert year to datetime.\n",
    "* Categorize genres properly: split strings into boolean columns per genre.\n",
    "* Modify the rating timestamp: from universal seconds to datetime year.\n",
    "* Check for NaN values. Clean (delete rows) if % of NaN values is small.\n",
    "\n",
    "We are going to use the Train and the Movies datasets. As we want to view the number of movies released on a yearly basis and also take a look at the ratings on a yearly basis as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Using the timestamp feature to generate the year and the month of the ratings'''\n",
    "\n",
    "train_yr_mnth =  train.copy()\n",
    "\n",
    "\n",
    "train_yr_mnth['year'] = train_yr_mnth['timestamp'].apply(lambda timestamp: datetime.fromtimestamp(timestamp).year)\n",
    "train_yr_mnth['month'] = train_yr_mnth['timestamp'].apply(lambda timestamp: datetime.fromtimestamp(timestamp).month)\n",
    "train_yr_mnth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = default_timer()\n",
    "\n",
    "'''We are using the movieId, title, genre and rating features from the Movies and Train datasets'''\n",
    "    \n",
    "'''Organise a bit and store into feather-format'''\n",
    "movies_df1.sort_values(by='movieId', inplace=True)\n",
    "movies_df1.reset_index(inplace=True, drop=True)\n",
    "train_df1.sort_values(by='movieId', inplace=True)\n",
    "train_df1.reset_index(inplace=True, drop=True)\n",
    "\n",
    "print(train_df1.dtypes)\n",
    "\n",
    "'''Split title and release year in separate columns in movies dataframe. Convert year to timestamp.'''\n",
    "movies_df1['year'] = movies_df1.title.str.extract(\"\\((\\d{4})\\)\", expand=True)\n",
    "movies_df1.year = pd.to_datetime(movies_df1.year, format='%Y')\n",
    "movies_df1.year = movies_df1.year.dt.year \n",
    "movies_df1.title = movies_df1.title.str[:-7]\n",
    "\n",
    "'''Categorize movies genres properly. '''\n",
    "genres_unique = pd.DataFrame(movies_df1.genres.str.split('|').tolist()).stack().unique()\n",
    "genres_unique = pd.DataFrame(genres_unique, columns=['genre']) # Format into DataFrame to store later\n",
    "movies_df1 = movies_df1.join(movies_df1.genres.str.get_dummies().astype(bool))\n",
    "movies_df1.drop('genres', inplace=True, axis=1)\n",
    "\n",
    "# Modify rating timestamp format (from seconds to datetime year)\n",
    "#ratings.timestamp = pd.to_datetime(ratings.timestamp, unit='s')\n",
    "#train_df1.timestamp = pd.to_datetime(train_df1.timestamp, infer_datetime_format=True)\n",
    "#train_df1.timestamp = train_df1.timestamp.dt.year\n",
    "\n",
    "\n",
    "print (\"Number of movies Null values: \", max(movies_df1.isnull().sum()))\n",
    "print (\"Number of ratings Null values: \", max(train_df1.isnull().sum()))\n",
    "movies_df1.dropna(inplace=True)\n",
    "train_df1.dropna(inplace=True)\n",
    "    \n",
    "'''Organise a bit, then save into feather-formatand clear from memory'''\n",
    "movies_df1.sort_values(by='movieId', inplace=True)\n",
    "train_df1.sort_values(by='movieId', inplace=True)\n",
    "movies_df1.reset_index(inplace=True, drop=True)\n",
    "train_df1.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "runtime = default_timer() - st\n",
    "print (\"Elapsed time(sec): \", round(runtime,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the graph for the above data that has been processed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = default_timer()\n",
    "\n",
    "'''Plotting the graph of the above processed data'''\n",
    "dftmp = movies_df1[['movieId', 'year']].groupby('year')\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(10,5))\n",
    "ax1.plot(dftmp.year.first(), dftmp.movieId.nunique(), \"g-o\")\n",
    "ax1.grid(None)\n",
    "ax1.set_ylim(0,)\n",
    "\n",
    "dftmp = train_df1[['rating', 'year']].groupby('year')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(dftmp.year.first(), dftmp.rating.count(), \"r-o\")\n",
    "ax2.grid(None)\n",
    "ax2.set_ylim(0,)\n",
    "\n",
    "ax1.set_xlabel('Year')\n",
    "ax1.set_ylabel('Number of movies released'); ax2.set_ylabel('Number of ratings')\n",
    "plt.title('Movies per year')\n",
    "plt.show()\n",
    "\n",
    "# Housekeeping\n",
    "%reset_selective -f (^dftmp$|^ax1$|^ax2$)\n",
    "\n",
    "runtime = default_timer() - st\n",
    "print (\"Elapsed time(sec): \", round(runtime,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of movies released per year increases almost exponentially until 2017, then flattening and dropping signifincantly thereafter each year. This could possibly mean that expontential growth (i.e. bubbles) is seldom sustainable in the long term.\n",
    "The ratings given by users only started in the year 1995 up until 2019. The ratings has been changing a lot some years the ratings increased some decreased but there has been a tremendous decrease in 2009 but then a tremendous increase in 2015 and then there after the ratings keep slightly decreasing each year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the graph below we are going to look at the number of overall ratings on a monthly basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1 = plt.subplots(figsize=(10,5))\n",
    "dftmp1 = train_df1[['rating', 'month']].groupby('month')\n",
    "ax2 = ax1.twinx()\n",
    "ax1.plot(dftmp1.month.first(), dftmp1.rating.count(), \"r-o\")\n",
    "ax2.grid(None)\n",
    "ax2.set_ylim(0,)\n",
    "\n",
    "ax1.set_xlabel('Month')\n",
    "#ax1.set_ylabel('Number of movies released'); ax2.set_ylabel('Number of ratings')\n",
    "ax1.set_ylabel('Number of ratings')\n",
    "plt.title('Movies per month')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph is showing that most ratings are received from the month of September up until November in a year, and then start decreasing in the month of December. The decrease is not significant, but that is maybe because users are busy with holiday commitments during that time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7 Insights on the imbd features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.1 Lets have a look at the budget feature of the imbd dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The imdb dataset seem to give us more insight with regards to the movies. We will be using both the train and the imdb dataset to see what the yearly budget was for the movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb_df = imdb.copy()\n",
    "imdb_df.head()\n",
    "\n",
    "'''removed special characters from the budget column and change its data type to float data type'''\n",
    "\n",
    "imdb_df['budget'].replace(regex=True, inplace=True, to_replace=r'[^0-9.\\-]',value=r'') \n",
    "imdb_df['budget'] = imdb_df['budget'].astype(float)\n",
    "\n",
    "'''merging the imdb and train table to see the overall yearly budget for all the movies'''\n",
    "budget_data = pd.merge(imdb_df, train_df1, on='movieId')\n",
    "\n",
    "year_mean = budget_data.groupby('year').mean()\n",
    "\n",
    "year_mean[['budget']].plot(title = 'Yearly Movie Budget',color=('DarkBlue'),linestyle=('-'),figsize=(10, 8))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Budget');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above graph we can see that the budget of the movies increases every year. This could be due to change of time, technology and other contributing factors that cost more on a yearly basis as time goes. So that would lead to cost of production costing more each year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **4.7.2.Runtime of movies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = pd.merge(imdb, movies, on='movieId')\n",
    "runtime.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 5 movies with longest running time\n",
    "runtime['runtime'] = runtime['runtime'].astype(float)\n",
    "b = runtime.drop(['movieId','title_cast', 'director', 'budget', 'plot_keywords', 'genres'], axis=1)\n",
    "b.head()\n",
    "b.nlargest(5,['runtime'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The longest movie is Taken (2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that people describe movies using words that are not too specific eg hero. Most people just use simple words like good or best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.7.3 Directors with most ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directors whose movies are the most rated\n",
    "b = pd.merge(train, imdb, on='movieId')\n",
    "mean_count = pd.DataFrame(b.groupby('director')['rating'].mean())\n",
    "mean_count['rating_counts'] = pd.DataFrame(b.groupby('director')['rating'].count())\n",
    "mean_count.nlargest(5,['rating_counts'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The director with most ratings is Quentin Tarantino with an average rating of 3.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Movie budgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Top 5 movies with highest budget\n",
    "budget = pd.merge(movies, imdb, on='movieId')\n",
    "budget['budget'].replace(regex=True, inplace=True, to_replace=r'[^0-9.\\-]',value=r'') \n",
    "budget['budget'] = budget['budget'].astype(float)\n",
    "z = budget.drop(['movieId','title_cast', 'director', 'runtime', 'plot_keywords', 'genres'], axis=1)\n",
    "z.head()\n",
    "z.nlargest(5,['budget'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The movie with the highest budget is My Way (Mai Wei) at 30 billion dollars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merging the imdb and train table to see the overall yearly budget for all the movies\n",
    "budget_data = pd.merge(budget, train_yr_mnth, on='movieId')\n",
    "budget_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_mean = budget_data.groupby('year').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_mean[['budget']].plot(title = 'Yearly Movie Budget',color=('DarkBlue'),linestyle=('-'),figsize=(10, 8))\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Budget');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above graph is illastrating that the budget of the movies increases every year. This could be due to change of time, technology and other contributing factors that cost more on a yearly basis as time goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the train and the imdb table to analyse any patterns between these two datasets regarding the movie rating and movie budget\n",
    "budget_rating = pd.merge(train, imdb_df, on='movieId')\n",
    "budget_rating.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "budget_rating.plot.scatter(x='rating', y='budget',title='Budget vs Rating Avg', color='DarkBlue', figsize=(6,5))\n",
    "plt.xlabel('Ratings')\n",
    "plt.ylabel('Budget');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The above graph is just showing us that the ratings on a movie does not really depend much on the budget of a movie. So a movie can have a high budget but still not get much ratings from user's.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two types of Recommender System\n",
    "\n",
    "- Collaborative Filtering Recommender System\n",
    "- Content-Based Recommender System\n",
    "\n",
    "#### Collaborative Filtering Recommender System\n",
    "Collaborative filtering recommender systems are based on the past interactions recorded between users and items in order to produce new recommendations. These interactions are stored in the so-called “user-item interactions matrix”.\n",
    "\n",
    "#### Advantages of Colaborative Filtering Recommender System\n",
    "- Works for any kind of item since no feature selection is needed\n",
    "- Requires not content analysis & extraction\n",
    "- Independent of any machine-readable representation of the objects being recommended\n",
    "- More diverse and serendipitous recommendation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Content-Based Recommender System\n",
    "Content based recommender system use additional information about users and/or items to make predictions.This additional information can be, for example, the age, the sex, the job or any other personal information for users.\n",
    "\n",
    "#### Advantages of Content-Based Recommender Sytem\n",
    "- Content representations are varied and they open up the options to use different approaches like: text processing techniques, the use of semantic information, inferences, etc…\n",
    "- It is easy to make a more transparent system: we use the same content to explain the recommendations.\n",
    "- We can avoid the “new item problem”\n",
    "#### Disadvantages of Content-Based Recommender Sytem\n",
    "- Content-Based RecSys tend to over-specialization: they will recommend items similar to those already consumed, with a tendecy of creating a “filter bubble”."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented a collaborative recommendation system. The methods based on Collaborative Filtering have shown to be, empirically, more precise when generating recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking a sample of 5% of the training data \n",
    "train = train.sample(frac =.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We had to take a sample of the data for traing in this notebook due to insufficient space here in kaggle to run a bigger fraction of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   This code is to build the trainset from train data. The reader() is used to read the file,\n",
    "   which returns an iterable reader object. The new data frame 'data' is to specify how to read the dataframe.\n",
    "   The build_full_trainset() is to format the data for use in the model fitting\n",
    "\"\"\"\n",
    "\n",
    "#reader() is used to read the file, which returns an iterable reader object.\n",
    "reader = Reader(rating_scale=(0.5,5.0))\n",
    "# It is to specify how to read the dataframe.\n",
    "data = Dataset.load_from_df(train[['userId', 'movieId', 'rating']], reader)\n",
    "# build the trainset from train data.., It is of dataset format from surprise library..\n",
    "trainset = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Singular Value Decomposition (SVD), a method from linear algebra that has been generally used as a dimensionality reduction technique in machine learning. SVD is a matrix factorisation technique, which reduces the number of features of a dataset by reducing the space dimension from N-dimension to K-dimension (where K<N). In the context of the recommender system, the SVD is used as a collaborative filtering technique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ Predicted Rating : __\n",
    "    - \n",
    "    - $ \\large  \\hat r_{ui} = \\mu + b_u + b_i + q_i^Tp_u $\n",
    "    \n",
    "        - $\\pmb q_i$ - Representation of item(movie) in latent factor space\n",
    "        \n",
    "        - $\\pmb p_u$ - Representation of user in new latent factor space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__\n",
    "    - \n",
    "    - $\\large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2\\right) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model using the SVD algorithm with the prepared trainset data from above chuck of code.\n",
    "svd = SVD()\n",
    "svd.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF stands for non-negative matrix factorization, a technique for obtaining low rank representation of matrices with non-negative or positive elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF decomposes multivariate data by creating a user-defined number of features. Each feature is a linear combination of the original attribute set; the coefficients of these linear combinations are non-negative.\n",
    "\n",
    "NMF decomposes a data matrix V into the product of two lower rank matrices W and H so that V is approximately equal to W times H. NMF uses an iterative procedure to modify the initial values of W and H so that the product approaches V. The procedure terminates when the approximation error converges or the specified number of iterations is reached.\n",
    "\n",
    "During model apply, an NMF model maps the original data into the new set of attributes (features) discovered by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import NMF\n",
    "#fitting the model using the NMF algorithm with the prepared trainset data from above chuck of code.\n",
    "nmf = NMF()\n",
    "nmf.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD++ algorithm, an extension of SVD taking into account implicit ratings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __ Predicted Rating : __\n",
    "    - \n",
    "    - $ \\large \\hat{r}_{ui} = \\mu + b_u + b_i + q_i^T\\left(p_u +\n",
    "    |I_u|^{-\\frac{1}{2}} \\sum_{j \\in I_u}y_j\\right) $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $ \\pmb{I_u}$ --- the set of all items rated by user u\n",
    "\n",
    "- $\\pmb{y_j}$ --- Our new set of item factors that capture implicit ratings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- __Optimization problem with user item interactions and regularization (to avoid overfitting)__\n",
    "    - \n",
    "    - $ \\large \\sum_{r_{ui} \\in R_{train}} \\left(r_{ui} - \\hat{r}_{ui} \\right)^2 +\n",
    "\\lambda\\left(b_i^2 + b_u^2 + ||q_i||^2 + ||p_u||^2 + ||y_j||^2\\right) $ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model using the SVD++ algorithm with the prepared trainset data from above chuck of code.\n",
    "svdpp = SVDpp()\n",
    "svdpp.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(svd, data,measures=['RMSE'], cv = 5, verbose = True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(nmf, data,measures=['RMSE'], cv = 5, verbose = True, n_jobs=-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_validate(svdpp, data,measures=['RMSE'], cv = 5, verbose = True,n_jobs=-1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "   This function takes an model, dataframe(df) which is the test, user id(uid) from the dataframe and item id(iid)\n",
    "   which in our case is movie id. It takes the test data and fit the model to make prediction of the ratings.\n",
    "   tqm is to see the prediction time or how the function has progressed in the data.\n",
    "\"\"\"\n",
    "\n",
    "def predicions(model, df, uid = 'userId', iid = 'movieId'):\n",
    "    from tqdm import tqdm\n",
    "    \n",
    "    return[\n",
    "        model.predict(uid=data[uid],iid =data[iid])[3]\n",
    "        for _, data in tqdm(test[[uid,iid]].iterrows(),total=len(test))\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using above function to make predictions with the svd model\n",
    "pred= predicions(svd, test, uid = 'userId', iid = 'movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using above function to make predictions with the nmf model\n",
    "pred0 = predicions(nmf, test, uid = 'userId', iid = 'movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using above function to make predictions with the svdpp model\n",
    "pred1 = predicions(svdpp, test, uid = 'userId', iid = 'movieId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This code creates a new column on the test dataframe with the userid and movieid merged together \"\"\"\n",
    "\n",
    "test['Id'] = test['userId'].map(str)+ \"_\" +test['movieId'].map(str)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sellecting the index of the test dataframe\n",
    "final_test= test[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the submission Dataframe\n",
    "Final_Table = {'Id': final_test, 'rating':np.round(pred, 1)}\n",
    "submission = pd.DataFrame(data=Final_Table)\n",
    "submission = submission[['Id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the submission Dataframe\n",
    "Final_Table0 = {'Id': final_test, 'rating':np.round(pred0, 1)}\n",
    "submission0 = pd.DataFrame(data=Final_Table)\n",
    "submission0 = submission0[['Id', 'rating']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the submission Dataframe\n",
    "Final_Table1 = {'Id': final_test, 'rating':np.round(pred1, 1)}\n",
    "submission1 = pd.DataFrame(data=Final_Table)\n",
    "submission1 = submission1[['Id', 'rating']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- saving submission to csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv(\"SvdSubmission.csv\",index  = False) #writing csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission0.to_csv(\"nmfSubmission.csv\",index  = False) #writing csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission1.to_csv(\"SvdppSubmission.csv\",index  = False) #wrting csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for Comet purposes\n",
    "#pre_comet = []\n",
    "#pre_comet_nmf = []\n",
    "pre_comet_svd = []\n",
    "for _, row in test_data.iterrows():\n",
    "    x_unseen = svd.predict(uid=row['userId'],iid=row['movieId'])\n",
    "    pred = x_unseen[3]\n",
    "    pre_comet_svd.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log metrics\n",
    "\n",
    "#experiment = Experiment(api_key=\"PXaqoLTlHzWF85kimEv0zHJPz\")\n",
    "with experiment.context_manager(\"validate\"):\n",
    "    #svd.fit(trainset)\n",
    "    #rmse_svdpp = mean_squared_error(test_data['rating'],pre_comet, squared=False)\n",
    "    #rmse_nmf = mean_squared_error(test_data['rating'],pre_comet_nmf, squared=False)\n",
    "    rmse_svd = mean_squared_error(test_data['rating'],pre_comet_nmf, squared=False)\n",
    "  # returns the validation accuracy\n",
    "    experiment.log_metric(\"rmse\", rmse_svd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the experiment\n",
    "\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comet experiment\n",
    "\n",
    "experiment.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONCLUSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best score we got when we used 50% of the data was 0.815. This is the score we submitted on the leaderboard. The best score came from fitting the model SVD Ffrom the surprise library. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}